{"paragraphs":[{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585905805716_-1017012869","id":"20200403-112325_2052129239","dateCreated":"2020-04-03T11:23:25+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7522","text":"import org.apache.spark.SparkContext._\nimport org.apache.spark.sql._\nimport org.apache.spark.rdd.RDD\n\nval inFile = spark.read.format(\"csv\")\n  .option(\"sep\", \",\")\n  .option(\"inferSchema\", \"true\")\n  .option(\"header\", \"true\")\n  .load(\"file:///home/julex/PDD/multiway_join.csv\") // R(A,B)\n  \nval inFile2 = spark.read.format(\"csv\")\n  .option(\"sep\", \",\")\n  .option(\"inferSchema\", \"true\")\n  .option(\"header\", \"true\")\n  .load(\"file:///home/julex/PDD/multiway_join_2.csv\") // S(B,C)\n\nval inFile3 = spark.read.format(\"csv\")\n  .option(\"sep\", \",\")\n  .option(\"inferSchema\", \"true\")\n  .option(\"header\", \"true\")\n  .load(\"file:///home/julex/PDD/multiway_join_3.csv\") // T(A,C)\n  \n  \ndef multiWayJoin(reducers: Int) = {\n    val r = inFile.count()\n    val s = inFile2.count()\n    val t = inFile3.count()\n\n    val k = reducers\n    val a = Math.cbrt(k * r * r / (s * t)).toInt\n    val b = Math.cbrt(k * s * s / (r * t)).toInt\n    val c = Math.cbrt(k * t * t / (r * s)).toInt\n    \n    val hashFun = (n: Int, count: Int) => n % count + 1\n    val hA = (n: Int) => hashFun(n, a)\n    val hB = (n: Int) => hashFun(n, b)\n    val hC = (n: Int) => hashFun(n, c)\n    \n    val mapFunR: (Row => Seq[((Int, Int, Int), (Int, Int, String))]) = (row: Row) => {\n          for {\n            w <- 1 to c\n          } yield ((hA(row.getInt(0)), hB(row.getInt(1)), w), (row.getInt(0), row.getInt(1), \"R\"))\n    }\n    \n    val mapFunS: (Row => Seq[((Int, Int, Int), (Int, Int, String))]) = (row: Row) => {\n          for {\n            u <- 1 to a\n          } yield ((u, hB(row.getInt(0)), hC(row.getInt(1))), (row.getInt(0), row.getInt(1), \"S\"))\n    }\n    \n    val mapFunT: (Row => Seq[((Int, Int, Int), (Int, Int, String))]) = (row: Row) => {\n          for {\n            v <- 1 to b\n          } yield ( (hA(row.getInt(0)), v, hC( row.getInt(1) ) ) , (row.getInt(0), row.getInt(1), \"T\")) \n    }\n    \n    val mappedR = inFile.flatMap(mapFunR)\n    val mappedS = inFile2.flatMap(mapFunS)\n    val mappedT = inFile3.flatMap(mapFunT)\n    val mappedAll = mappedR.union(mappedS).union(mappedT).rdd.cache\n    \n    val groupRecordsFromOneRelation: ( (((Int, Int, Int), List[(Int, Int, String)])) => ( (Int, Int, Int), ( List[(Int, Int)], List[(Int, Int)], List[(Int, Int)]))) = \n        (in: (((Int, Int, Int), List[(Int, Int, String)])) ) => {\n        var rTuples = List[(Int, Int)]()\n        var sTuples = List[(Int, Int)]()\n        var tTuples = List[(Int, Int)]()\n        \n        for (tuple <- in._2) {\n            if (tuple._3 == \"R\") {\n                rTuples = (tuple._1, tuple._2) :: rTuples\n            }\n            else if (tuple._3 == \"S\") {\n                sTuples = (tuple._1, tuple._2) :: sTuples\n            }\n            else {\n                tTuples = (tuple._1, tuple._2) :: tTuples\n            }\n        }\n        \n        (in._1, (rTuples, sTuples, tTuples))\n    }\n    \n    val joinFunc: ( (((Int, Int, Int), ( List[(Int, Int)], List[(Int, Int)], List[(Int, Int)]))) => ( (Int, Int, Int), List[(Int, Int, Int)] ) ) = \n        (in: (((Int, Int, Int), ( List[(Int, Int)], List[(Int, Int)], List[(Int, Int)])))) => {\n        \n        var tuples = List[(Int, Int, Int)]()\n        \n        for (tupleR <- in._2._1) {\n            for (tupleS <- in._2._2) {\n                if (tupleR._2 == tupleS._1) {\n                    tuples = (tupleR._1, tupleR._2, tupleS._2) :: tuples\n                }\n            }\n        }\n        \n        // filter depending on existance in T relation\n        (in._1, tuples.filter(tuple => in._2._3.contains((tuple._1, tuple._3))))\n    }\n    \n    val groupedRelations = mappedAll.groupByKey().map(x => (x._1, x._2.toList)).map(groupRecordsFromOneRelation).cache\n    val result = groupedRelations.map(joinFunc).flatMap(x => x._2).collect()\n    result.mkString(\"\\n\")\n}\n\n\n// USAGE\nmultiWayJoin(10)","dateUpdated":"2020-04-03T11:23:28+0200","dateFinished":"2020-04-03T11:23:34+0200","dateStarted":"2020-04-03T11:23:28+0200","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.SparkContext._\nimport org.apache.spark.sql._\nimport org.apache.spark.rdd.RDD\ninFile: org.apache.spark.sql.DataFrame = [A: int, B: int]\ninFile2: org.apache.spark.sql.DataFrame = [B: int, C: int]\ninFile3: org.apache.spark.sql.DataFrame = [A: int, C: int]\nmultiWayJoin: (reducers: Int)String\nres99: String =\n(5,4,1)\n(5,4,2)\n(4,3,1)\n(6,4,1)\n(5,3,1)\n(5,3,2)\n(6,4,2)\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585905808899_-563366290","id":"20200403-112328_538663697","dateCreated":"2020-04-03T11:23:28+0200","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7602"}],"name":"MultiWay_Join","id":"2F4ESC2VV","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}